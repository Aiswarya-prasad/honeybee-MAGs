#!/usr/bin/env python

"""
name: honeybee-MAGs-pipeline
description: Snakefile to be used to launch the pipeline and run qc, assembly, binning and some downstream steps
author: Aiswarya Prasad (aiswarya.prasad@unil.ch)
dependencies:
    - raw data
    - config files
"""

import os
import glob
import yaml
import itertools
from itertools import chain

configfile: "config/config.yaml"
# read config information into local variables to improve readability of rules
if config["LocalBackup"]:
    localrules: backup

SAMPLES_KE = config["SAMPLES_KE"]
SAMPLES_INDIA = config["SAMPLES_INDIA"]
SAMPLES_MY = config["SAMPLES_MY"]
SAMPLES = SAMPLES_KE + SAMPLES_INDIA + SAMPLES_MY
# SAMPLES = config["SAMPLES_KE"]
ADAPTERS = config["Adapters"]

wildcard_constraints:
  sample = '|'.join(SAMPLES),
#   read = '|'.join(["R1", "R2"]),
#   lane = "L*",
  run = "20[0-9]{6,6}"

onstart:
    # this is just for updates - needs to be run before starting the pipeline
    shell("python3 scripts/make_reads_list_file.py")

raw_paths_dict_all = yaml.safe_load(open("config/raw_file_paths.yaml", "r"))
raw_paths_dict = {key: raw_paths_dict_all[key] for key in SAMPLES}

include: "common.smk"

# samples that failed in the first run (250):
LARGE_SAMPLES = ["A2-2", "A2-3", "A3-4", "A4-4", "A6-4", "D1-2", "D2-1", "D2-2", "D2-4", "D2-5", "D3-2", "D9-5", "F2-5", "F3-4", "F3-5", "F4-1", "F7-5", "F8-2", "F8-4"]
# samples that failed in the first run (450 - 800) ("A6-4", "D1-2", "D3-2" were completed with the higher RAM):
LARGE_SAMPLES = ["A2-2", "A2-3", "A3-4", "A4-4", "D2-1", "D2-2", "D2-4", "D2-5", "D9-5", "F2-5", "F3-4", "F3-5", "F4-1", "F7-5", "F8-2", "F8-4"]

rule targets:
    input:
        # raw_files = get_all_input_files(raw_paths_dict),
        html_qc_raw = [f"results/00_rawreads/fastqc/{x.split('.fastq.gz')[0]}_fastqc.html" for x in get_list_of_values(get_renamed_input_files(raw_paths_dict))],
        # all_reads_assemblies = [f"results/00_trimmedreads/{x.split('.fastq.gz')[0]}_trim.fastq.gz" for x in get_list_of_values(get_renamed_input_files(raw_paths_dict))],
        html_qc_trim = [f"results/00_trimmedreads/fastqc/{x.split('.fastq.gz')[0]}_trim_fastqc.html" for x in get_list_of_values(get_renamed_input_files(raw_paths_dict))],
        concat_reads = expand("results/01_trimmedconcatreads/{sample}_{read}.fastq.gz", sample = SAMPLES, read = ["R1", "R2"]),
        motus_merged = "results/02_motus_profile/samples_merged.motus",
        assembly_mapping = "results/05_assembly/scaffolds_mapping/Assembly_mapping_summary.csv",
        orfs = expand("results/06_metagenomicORFs/{sample}_orfs.ffn", sample = SAMPLES_INDIA+SAMPLES_MY),
        # just here for now to make sure cleaning reads is done for all of the samples
        singletons = expand("results/01_cleanreads/{sample}_singletons.fastq.gz", sample = SAMPLES)
# but 06 inside results and rename 05_assembly/all_reads_assemblies
include: "trim-qc.smk"
include: "motus-profiling.smk"
include: "assemble-qc.smk"

# to do next, 
# binning
# drep and gtdb ..
# rename MAGs (with completeness and genus in the name?) and checkpoint table
# annotation
# phylogenies
# core coverage
# sdp validation
# popcogent