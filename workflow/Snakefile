#!/usr/bin/env python

"""
name: honeybee-MAGs-pipeline
description: Snakefile to be used to launch the pipeline and run qc, assembly, binning and some downstream steps
author: Aiswarya Prasad (aiswarya.prasad@unil.ch)
dependencies:
    - raw data
    - config files
"""

import os
import glob
import yaml
import itertools
import subprocess
from itertools import chain

configfile: "config/config.yaml"
# read config information into local variables to improve readability of rules
if config["LocalBackup"]:
    localrules: backup

SAMPLES_sub = config["SAMPLES_subset"]
SAMPLES_KE = config["SAMPLES_KE"]
SAMPLES_INDIA = config["SAMPLES_INDIA"]
SAMPLES_MY = config["SAMPLES_MY"]
SAMPLES = SAMPLES_KE + SAMPLES_INDIA + SAMPLES_MY
# SAMPLES = config["SAMPLES_KE"]
ADAPTERS = config["Adapters"]

wildcard_constraints:
  sample = '|'.join(SAMPLES),
#   read = '|'.join(["R1", "R2"]),
#   lane = "L*",
  run = "20[0-9]{6,6}"

onstart:
    # this is just for updates - needs to be run before starting the pipeline
    shell("python3 scripts/make_reads_list_file.py")

raw_paths_dict_all = yaml.safe_load(open("config/raw_file_paths.yaml", "r"))
raw_paths_dict = {key: raw_paths_dict_all[key] for key in SAMPLES}

include: "common.smk"

# samples that failed in the first run (250):
LARGE_SAMPLES = ["A2-2", "A2-3", "A3-4", "A4-4", "A6-4", "D1-2", "D2-1", "D2-2", "D2-4", "D2-5", "D3-2", "D9-5", "F2-5", "F3-4", "F3-5", "F4-1", "F7-5", "F8-2", "F8-4"]
# samples that failed in the first run (450 - 800) ("A6-4", "D1-2", "D3-2" were completed with the higher RAM):
LARGE_SAMPLES = ["A2-2", "A2-3", "A3-4", "A4-4", "D2-1", "D2-2", "D2-4", "D2-5", "D9-5", "F2-5", "F3-4", "F3-5", "F4-1", "F7-5", "F8-2", "F8-4"]

rule targets:
    input:
        # raw_files = get_all_input_files(raw_paths_dict),
        # html_qc_raw = [f"results/00_rawreads/fastqc/{x.split('.fastq.gz')[0]}_fastqc.html" for x in get_list_of_values(get_renamed_input_files(raw_paths_dict))],
        # all_reads_assemblies = [f"results/00_trimmedreads/{x.split('.fastq.gz')[0]}_trim.fastq.gz" for x in get_list_of_values(get_renamed_input_files(raw_paths_dict))],
        # html_qc_trim = [f"results/00_trimmedreads/fastqc/{x.split('.fastq.gz')[0]}_trim_fastqc.html" for x in get_list_of_values(get_renamed_input_files(raw_paths_dict))],
        # concat_reads = expand("results/01_trimmedconcatreads/{sample}_{read}.fastq.gz", sample = SAMPLES, read = ["R1", "R2"]),
        reads1 = expand("results/01_cleanreads/{sample}_R1_repaired.fastq.gz", sample = SAMPLES),
        reads2 = expand("results/01_cleanreads/{sample}_R2_repaired.fastq.gz", sample = SAMPLES),
        # # Mapping to DBs - done but only kept in backup
        # flagstat_hM = expand("results/04_MapToDBs/{sample}/{sample}_host_unmapd_map_MAGs_rep.flagstat", sample=SAMPLES_INDIA+SAMPLES_MY),
        # reads1_host_unmpd_MAGs_rep_unmapd = expand("results/04_MapToDBs/{sample}/{sample}_R1_host_unmapd_map_MAGs_rep_unmapped.fastq.gz", sample=SAMPLES_INDIA+SAMPLES_MY),
        # reads2_host_unmpd_MAGs_rep_unmapd = expand("results/04_MapToDBs/{sample}/{sample}_R2_host_unmapd_map_MAGs_rep_unmapped.fastq.gz", sample=SAMPLES_INDIA+SAMPLES_MY),
        # flagstat_Mh = expand("results/04_MapToDBs/{sample}/{sample}_unmapd_rep_MAGs_host_mapping.flagstat", sample=SAMPLES_INDIA+SAMPLES_MY),
        # reads1_MAGs_rep_unmpd_host_unmapd = expand("results/04_MapToDBs/{sample}/{sample}_R1_unmapd_rep_MAGs_host_unmapd.fastq.gz", sample=SAMPLES_INDIA+SAMPLES_MY),
        # reads2_MAGs_rep_unmpd_host_unmapd = expand("results/04_MapToDBs/{sample}/{sample}_R2_unmapd_rep_MAGs_host_unmapd.fastq.gz", sample=SAMPLES_INDIA+SAMPLES_MY),
        # reads1_unmapd_rep_MAGs = expand("results/04_MapToDBs/{sample}/{sample}_R1_unmapd_rep_MAGs.fastq.gz", sample=SAMPLES_INDIA+SAMPLES_MY),
        # reads2_unmapd_rep_MAGs = expand("results/04_MapToDBs/{sample}/{sample}_R2_unmapd_rep_MAGs.fastq.gz", sample=SAMPLES_INDIA+SAMPLES_MY),
        flagstat = expand("results/03_host_mapping/{sample}_flagstat.txt", sample=SAMPLES_INDIA+SAMPLES_MY),
        # # Motus - done but only kept in backup
        # motus_merged = "results/02_motus_profile/samples_merged.motus",
        # assembly
        scaffolds = expand("results/05_assembly/all_reads_assemblies/{sample}_scaffolds.fasta", sample = SAMPLES_INDIA+SAMPLES_MY),
        spades_log = expand("results/05_assembly/all_reads_assemblies/{sample}_spades.log", sample = SAMPLES_INDIA+SAMPLES_MY),
        graph = expand("results/05_assembly/all_reads_assemblies/{sample}_assembly_graph.fastg", sample = SAMPLES_INDIA+SAMPLES_MY),
        assembly_summary = "results/05_assembly/assembly_summary.txt",
        # sort filter orfs
        orfs = expand("results/06_metagenomicORFs/{sample}/filt_orfs/{sample}.ffn", sample = SAMPLES_INDIA+SAMPLES_MY),
        ### ! If running backmapping make sure to include a cap on number of --jobs in the snakemake command do <50 is safe do not do >100 ! (40,000 jobs for 200 samples) ###
        # qualimap_results = expand("results/07_MAG_binng_QC/01_backmapping/qualimap_results/{sample_assembly}/", sample_assembly = SAMPLES_INDIA+SAMPLES_MY),
        checkm_plots = expand("results/07_MAG_binng_QC/03_checkm_results/{sample}/plots.done", sample=SAMPLES_INDIA+SAMPLES_MY),
        checkm_summary = "results/09_MAGs_collection/checkm_merged.tsv",
        gene_profiling = expand("results/08_gene_content/01_profiling_bowtie2/{sample}_mapped.hist", sample=SAMPLES_INDIA+SAMPLES_MY),
        count_genes = expand("results/08_gene_content/01_profiling_bowtie2/{sample}_gene_coverage.txt", sample=SAMPLES_INDIA+SAMPLES_MY),
        dram_annotation = expand("results/08_gene_content/02_DRAM_annotations/{sample}/annotations.tsv", sample=SAMPLES_INDIA+SAMPLES_MY),
        mag_metadata = "results/09_MAGs_collection/MAGs_metadata_summary.tsv",
        profile_plots = expand("results/10_instrain/04_instrain_plot_marker/{sample}_profile_plots.done", sample=SAMPLES),
        # compare_plots = "results/10_instrain/03_instrain_compare/all_compared.done",
        # 
        # orthofinder_out = lambda wildcards: expand("results/11_phylogenies/02_orthofinder_results/{genus}/Results_{genus}/Species_Tree/SpeciesTree_rooted_node_labels.txt", genus = get_significant_genera(checkpoints.mag_metadata_summary.get().output.metadata)),
        # orthofinder_out_iqtree = lambda wildcards: expand("results/11_phylogenies/02_orthofinder_results/{genus}/Results_{genus}_iqtree/Species_Tree/SpeciesTree_rooted_node_labels.txt", genus = get_significant_genera(checkpoints.mag_metadata_summary.get().output.metadata)),
        kraken = expand("results/05_assembly/contig_fates/kraken2/{sample}_report.txt", sample=SAMPLES_INDIA+SAMPLES_MY),
        kaiju = expand("results/05_assembly/contig_fates/kaiju/nr/{sample}_names.txt", sample=SAMPLES_INDIA+SAMPLES_MY),
        kaiju_full = expand("results/05_assembly/contig_fates/kaiju/nr/{sample}_fullnames.txt", sample=SAMPLES_INDIA+SAMPLES_MY),
        kaiju_genes = "results/08_gene_content/04_kaiju_on_genes/nr/20230313_gene_catalog_taxa.txt",
        kaiju_genes_full = "results/08_gene_content/04_kaiju_on_genes/nr/20230313_gene_catalog_taxa_full.txt",
        kraken_genes = "results/08_gene_content/04_kraken2_on_genes/20230313_gene_catalog_report.txt",
        # contig_fates = "results/05_assembly/contig_fates/{sample}_contig_fates.tsv",
        bowtie_2_index = multiext("results/10_instrain/00_prepare_mags/mag_rep_database.fa", ".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.2.bt2"),
        cayman_profile = expand("results/08_gene_content/06_cayman/{sample}.cazy.txt.gz", sample = SAMPLES),
        tree = lambda wildcards: expand("results/11_phylogenies/03_iqtree_trees/{genus}.done", genus = get_significant_genera(checkpoints.mag_metadata_summary.get().output.metadata))


include: "trim-qc.smk"
include: "motus-profiling.smk"
include: "assemble-qc.smk"
include: "backmapping-binning.smk"
include: "binning_summary_annotation.smk"
include: "annotate_profile_orfs.smk"
include: "mag_db_instrain.smk"
include: "core_pan_genome.smk"
include: "mag_phylogenies.smk"

# to do next, 
# annotation
# phylogenies
# core coverage
# sdp validation
# popcogent

# create rule to run assembly and binning for all samples

